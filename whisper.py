
#    ‚ïî‚ïó‚ïî‚îå‚îÄ‚îê‚î¨‚îÄ‚îê‚îå‚îÄ‚îê‚î¨ ‚î¨
#    ‚ïë‚ïë‚ïë‚îú‚î§ ‚îú‚î¨‚îò‚îÇ  ‚îî‚î¨‚îò
#    ‚ïù‚ïö‚ïù‚îî‚îÄ‚îò‚î¥‚îî‚îÄ‚îî‚îÄ‚îò ‚î¥ 

# Code is licensed under CC-BY-NC-ND 4.0 unless otherwise specified.
# https://creativecommons.org/licenses/by-nc-nd/4.0/
# You CANNOT edit this file without direct permission from the author.
# You can redistribute this file without any changes.

# meta developer: @nercymods
# scope: hikka_min 1.6.2

import os
import openai
from pydub import AudioSegment
import json

from hikkatl.types import Message
from .. import loader, utils


@loader.tds
class WhisperMod(loader.Module):
    """Module for speech recognition"""
    strings = {
        "name": "WhisperMod",
        "audio_not_found": "<b><emoji document_id=5818678700274617758>üëÆ‚Äç‚ôÄÔ∏è</emoji>Not found to recognize.</b>",
        "recognized": "<b><emoji document_id=5821302890932736039>üó£</emoji>Recognized:</b>\n{transcription}",
        "error": "<b><emoji document_id=5980953710157632545>‚ùå</emoji>Error occurred during transcription.</b>",
        "recognition": "<b><emoji document_id=5307937750828194743>ü´•</emoji>Recognition...</b>",
        "downloading": "Downloading, wait",
        "autowhisper_enabled": "<b><emoji document_id=5307937750828194743>ü´•</emoji>Auto-whisper enabled in this chat.</b>",
        "autowhisper_disabled": "<b><emoji document_id=5307937750828194743>ü´•</emoji>Auto-whisper disabled in this chat.</b>"
    }

    strings_ru = {
        "audio_not_found": "<b><emoji document_id=5818678700274617758>üëÆ‚Äç‚ôÄÔ∏è</emoji>–ù–µ –Ω–∞–π–¥–µ–Ω–æ, —á—Ç–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å.</b>",
        "recognized": "<b><emoji document_id=5821302890932736039>üó£</emoji>–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ:</b>\n{transcription}",
        "error": "<b><emoji document_id=5980953710157632545>‚ùå</emoji>–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.</b>",
        "recognition": "<b><emoji document_id=5307937750828194743>ü´•</emoji>–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...</b>",
        "downloading": "<b><emoji document_id=5310189005181036109>üêç</emoji>–°–∫–∞—á–∏–≤–∞–Ω–∏–µ, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...</b>",
        "autowhisper_enabled": "<b><emoji document_id=5307937750828194743>ü´•</emoji>–ê–≤—Ç–æ—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≤–∫–ª—é—á–µ–Ω–æ –≤ —ç—Ç–æ–º —á–∞—Ç–µ.</b>",
        "autowhisper_disabled": "<b><emoji document_id=5307937750828194743>ü´•</emoji>–ê–≤—Ç–æ—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ —ç—Ç–æ–º —á–∞—Ç–µ.</b>"
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "api_key",
                None,
                lambda: "Api key for Whisper",
                validator=loader.validators.Hidden(),
            ),
            loader.ConfigValue(
                "temperature",
                '0.2',
                lambda: "The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.",
                validator=loader.validators.String(),         
            ),
            loader.ConfigValue(
                "prompt",
                None,
                lambda: "An optional text to guide the model's style or continue a previous audio segment. The prompt should match the audio language.",
                validator=loader.validators.String(),
            )
        )

    async def client_ready(self, client, db):
        self.db = db
        self.client = client

    @loader.command(ru_doc="—Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ/–≤–∏–¥–µ–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ä–µ–ø–ª–∞–µ")
    async def whisper(self, m: Message):
        """Transcribe speech from a voice/video message in reply"""
        rep = await m.get_reply_message()
        await m.delete()

        down = await rep.reply(self.strings["downloading"])
        file = await rep.download_media()
        file_extension = os.path.splitext(file)[1].lower()

        openai.api_key = self.config["api_key"]

        if file_extension == ".oga" or file_extension == ".ogg":
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognition"])
            input_file = file

            audio = AudioSegment.from_file(input_file, format="ogg")
            audio.export("output_file.mp3", format="mp3")

            audio_file = open("output_file.mp3", "rb")
            response = openai.Audio.transcribe("whisper-1", audio_file, prompt=self.config["prompt"], temperature = self.config["temperature"])
            response_dict = response.to_dict()
            transcription = response_dict['text']
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognized"].format(transcription=transcription))
            os.remove(file)
            os.remove("output_file.mp3")            

        elif file_extension == ".mp3" or file_extension == "m4a" or file_extension == ".wav" or file_extension == ".mpeg" or file_extension == ".mp4":
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognition"])
            input_file = file

            audio_file = open(input_file, "rb")
            response = openai.Audio.transcribe("whisper-1", audio_file, prompt=self.config["prompt"], temperature = self.config["temperature"])
            response_dict = response.to_dict()
            transcription = response_dict['text']
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognized"].format(transcription=transcription))
            os.remove(file)

        else:
            await utils.answer(m, self.strings["audio_not_found"])

    @loader.command(ru_doc="–≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∏ –≤–∏–¥–µ–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —á–∞—Ç–µ –≥–¥–µ –≤–≤–µ–¥–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞")
    async def autowhspr(self, m: Message):
        """Enable/disable auto-speech recognition for voice and video messages"""
        chat_id = str(m.chat_id)
        current_state = self.db.get("WhisperMod", "autowhspr", {})
        enabled = current_state.get(chat_id, False)

        if enabled:
            current_state.pop(chat_id, None)
            status_message = self.strings["autowhisper_disabled"]
        else:
            current_state[chat_id] = True
            status_message = self.strings["autowhisper_enabled"]

        self.db.set("WhisperMod", "autowhspr", current_state)
        await utils.answer(m, status_message)

    @loader.watcher(only_media=True)
    async def autowhisper_watcher(self, message: Message):
        """Watcher to automatically transcribe voice and video messages when auto-speech recognition is enabled"""
        chat_id = str(message.chat_id)
        current_state = self.db.get("WhisperMod", "autowhspr", {})

        if current_state.get(chat_id, False):
            if message.voice or message.video:
                if not message.gif and not message.sticker and not message.photo:
                    rep = message
                    await self.whisperwatch(rep)


    async def whisperwatch(self, m: Message):
        """Transcribe speech from a voice/video message in reply"""
        rep = m

        down = await rep.reply(self.strings["downloading"])
        file = await rep.download_media()
        file_extension = os.path.splitext(file)[1].lower()


        openai.api_key = self.config["api_key"]

        if file_extension == ".oga" or file_extension == ".ogg":
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognition"])
            input_file = file

            audio = AudioSegment.from_file(input_file, format="ogg")
            audio.export("output_file.mp3", format="mp3")

            audio_file = open("output_file.mp3", "rb")
            response = openai.Audio.transcribe("whisper-1", audio_file, prompt=self.config["prompt"], temperature = self.config["temperature"])
            response_dict = response.to_dict()
            transcription = response_dict['text']
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognized"].format(transcription=transcription))
            os.remove(file)
            os.remove("output_file.mp3")

        elif file_extension == ".mp3" or file_extension == "m4a" or file_extension == ".wav" or file_extension == ".mpeg" or file_extension == ".mp4":
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognition"])
            input_file = file

            audio_file = open(input_file, "rb")
            response = openai.Audio.transcribe("whisper-1", audio_file, prompt=self.config["prompt"], temperature = self.config["temperature"])
            response_dict = response.to_dict()
            transcription = response_dict['text']
            await self.client.edit_message(m.chat_id, down.id, self.strings["recognized"].format(transcription=transcription))
            os.remove(file)

        else:
            await utils.answer(m, self.strings["audio_not_found"])
