#    ‚ïî‚ïó‚ïî‚îå‚îÄ‚îê‚î¨‚îÄ‚îê‚îå‚îÄ‚îê‚î¨ ‚î¨
#    ‚ïë‚ïë‚ïë‚îú‚î§ ‚îú‚î¨‚îò‚îÇ  ‚îî‚î¨‚îò
#    ‚ïù‚ïö‚ïù‚îî‚îÄ‚îò‚î¥‚îî‚îÄ‚îî‚îÄ‚îò ‚î¥

# Code is licensed under CC-BY-NC-ND 4.0 unless otherwise specified.
# https://creativecommons.org/licenses/by-nc-nd/4.0/
# You CANNOT edit this file without direct permission from the author.
# You can redistribute this file without any changes.

# meta developer: @nercymods
# scope: hikka_min 1.6.2
# requires: pydub openai==1.3.8 ffmpeg

import os

import openai
from hikkatl.tl.types import Message
from pydub import AudioSegment

from .. import loader, utils


@loader.tds
class WhisperMod(loader.Module):
    """Module for speech recognition"""

    strings = {
        "name": "WhisperMod",
        "audio_not_found": (
            "<b><emoji document_id=5818678700274617758>üëÆ‚Äç‚ôÄÔ∏è</emoji>Not found to"
            " recognize.</b>"
        ),
        "recognized": (
            "<b><emoji"
            " document_id=5821302890932736039>üó£</emoji>Recognized:</b>\n{transcription}"
        ),
        "error": (
            "<b><emoji document_id=5980953710157632545>‚ùå</emoji>Error occurred during"
            " transcription.</b>"
        ),
        "recognition": (
            "<b><emoji document_id=5307937750828194743>ü´•</emoji>Recognition...</b>"
        ),
        "downloading": "<b><emoji document_id=5310189005181036109>üêç</emoji>Downloading, wait</b>",
        "autowhisper_enabled": (
            "<b><emoji document_id=5307937750828194743>ü´•</emoji>Auto-whisper enabled"
            " in this chat.</b>"
        ),
        "autowhisper_disabled": (
            "<b><emoji document_id=5307937750828194743>ü´•</emoji>Auto-whisper disabled"
            " in this chat.</b>"
        ),
        "no_api": "<b><emoji document_id=5980953710157632545>‚ùå</emoji> Insert openai api-key in config</b> (<code>.cfg whispermod</code>)",
        "invalid_key": "<b><emoji document_id=5980953710157632545>‚ùå</emoji> Invalid openai api-key</b>",
    }

    strings_ru = {
        "audio_not_found": (
            "<b><emoji document_id=5818678700274617758>üëÆ‚Äç‚ôÄÔ∏è</emoji>–ù–µ –Ω–∞–π–¥–µ–Ω–æ, —á—Ç–æ"
            " —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å.</b>"
        ),
        "recognized": (
            "<b><emoji"
            " document_id=5821302890932736039>üó£</emoji>–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ:</b>\n{transcription}"
        ),
        "error": (
            "<b><emoji document_id=5980953710157632545>‚ùå</emoji>–û—à–∏–±–∫–∞ –ø—Ä–∏"
            " —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.</b>"
        ),
        "recognition": (
            "<b><emoji document_id=5307937750828194743>ü´•</emoji>–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ...</b>"
        ),
        "downloading": (
            "<b><emoji document_id=5310189005181036109>üêç</emoji>–°–∫–∞—á–∏–≤–∞–Ω–∏–µ,"
            " –ø–æ–¥–æ–∂–¥–∏—Ç–µ...</b>"
        ),
        "autowhisper_enabled": (
            "<b><emoji document_id=5307937750828194743>ü´•</emoji>–ê–≤—Ç–æ—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"
            " –≤–∫–ª—é—á–µ–Ω–æ –≤ —ç—Ç–æ–º —á–∞—Ç–µ.</b>"
        ),
        "autowhisper_disabled": (
            "<b><emoji document_id=5307937750828194743>ü´•</emoji>–ê–≤—Ç–æ—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"
            " –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ —ç—Ç–æ–º —á–∞—Ç–µ.</b>"
        ),
        "no_api": "<b><emoji document_id=5980953710157632545>‚ùå</emoji> –£–∫–∞–∂–∏—Ç–µ api-–∫–ª—é—á –≤ –∫–æ–Ω—Ñ–∏–≥–µ</b> (<code>.cfg whispermod</code>)",
        "invalid_key": "<b><emoji document_id=5980953710157632545>‚ùå</emoji> –ù–µ–≤–µ—Ä–Ω—ã–π api-–∫–ª—é—á</b>",
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "api_key",
                None,
                lambda: "Api key for Whisper (https://platform.openai.com/account/api-keys)",
                validator=loader.validators.Hidden(),
            ),
            loader.ConfigValue(
                "temperature",
                "0.2",
                lambda: (
                    "The sampling temperature, between 0 and 1. Higher values like 0.8"
                    " will make the output more random, while lower values like 0.2"
                    " will make it more focused and deterministic. If set to 0, the"
                    " model will use log probability to automatically increase the"
                    " temperature until certain thresholds are hit."
                ),
                validator=loader.validators.String(),
            ),
            loader.ConfigValue(
                "prompt",
                None,
                lambda: (
                    "An optional text to guide the model's style or continue a previous"
                    " audio segment. The prompt should match the audio language."
                ),
                validator=loader.validators.String(),
            ),
        )

    @loader.command(ru_doc="—Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å –∏–∑ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ/–≤–∏–¥–µ–æ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ä–µ–ø–ª–∞–µ")
    async def whisper(self, message: Message):
        """Transcribe speech from a voice/video message in reply"""
        if self.config["api_key"] is None:
            await utils.answer(message, self.strings["no_api"])
            return
        rep = await message.get_reply_message()
        down = await utils.answer(message, self.strings["downloading"])
        file = await rep.download_media()
        file_extension = os.path.splitext(file)[1].lower()

        if file_extension in [".oga", ".ogg"]:
            await self.client.edit_message(
                message.chat_id, down.id, self.strings["recognition"]
            )
            input_file = file

            audio = AudioSegment.from_file(input_file, format="ogg")
            audio.export("output_file.mp3", format="mp3")

            audio_file = open("output_file.mp3", "rb")

            client = openai.AsyncOpenAI(api_key=self.config["api_key"])
            try:
                response = await client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    prompt=self.config["prompt"],
                    temperature=self.config["temperature"],
                )
            except openai.AuthenticationError:
                await utils.answer(message, self.strings["invalid_key"])
                return
            except Exception as e:
                await utils.answer(
                    message,
                    f"<b><emoji document_id=5980953710157632545>‚ùå</emoji>Error: {e}</b>",
                )
                return
            transcription = response.text
            await self.client.edit_message(
                message.chat_id,
                down.id,
                self.strings["recognized"].format(transcription=transcription),
            )
            os.remove(file)
            os.remove("output_file.mp3")
        elif file_extension in [".mp3", "m4a", ".wav", ".mpeg", ".mp4"]:
            await self.client.edit_message(
                message.chat_id, down.id, self.strings["recognition"]
            )
            input_file = file

            audio_file = open(input_file, "rb")

            client = openai.AsyncOpenAI(api_key=self.config["api_key"])
            try:
                response = await client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    prompt=self.config["prompt"],
                    temperature=self.config["temperature"],
                )
            except openai.AuthenticationError:
                await utils.answer(message, self.strings["invalid_key"])
                return
            except Exception as e:
                await utils.answer(
                    message,
                    f"<b><emoji document_id=5980953710157632545>‚ùå</emoji>Error: {e}</b>",
                )
                return
            transcription = response.text
            await self.client.edit_message(
                message.chat_id,
                down.id,
                self.strings["recognized"].format(transcription=transcription),
            )
            os.remove(file)
            os.remove("output_file.mp3")
        else:
            await utils.answer(message, self.strings["audio_not_found"])

    @loader.command(
        ru_doc=(
            "–≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –∞–≤—Ç–æ—Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –≥–æ–ª–æ—Å–æ–≤—ã—Ö –∏ –≤–∏–¥–µ–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —á–∞—Ç–µ"
            " –≥–¥–µ –≤–≤–µ–¥–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞"
        )
    )
    async def autowhspr(self, message: Message):
        """Enable/disable auto-speech recognition for voice and video messages"""
        chat_id = str(message.chat_id)
        current_state = self.get("autowhspr", {})
        enabled = current_state.get(chat_id, False)

        if enabled:
            current_state.pop(chat_id, None)
            status_message = self.strings["autowhisper_disabled"]
        else:
            current_state[chat_id] = True
            status_message = self.strings["autowhisper_enabled"]
        self.set("autowhspr", current_state)
        await utils.answer(message, status_message)

    @loader.watcher(only_media=True)
    async def autowhisper_watcher(self, message: Message):
        """Watcher to automatically transcribe voice and video messages when auto-speech recognition is enabled"""
        chat_id = str(message.chat_id)
        current_state = self.get("autowhspr", {})

        if current_state.get(chat_id, False):
            if message.voice or message.video:
                if not message.gif and not message.sticker and not message.photo:
                    rep = message
                    await self.whisperwatch(rep)

    async def whisperwatch(self, message: Message):
        """Transcribe speech from a voice/video message in reply"""
        rep = message
        down = await self.client.send_message(
            message.chat.id, message=self.strings["downloading"], reply_to=rep.id
        )
        file = await rep.download_media()
        file_extension = os.path.splitext(file)[1].lower()

        if file_extension in [".oga", ".ogg"]:
            await self.client.edit_message(
                message.chat_id, down.id, self.strings["recognition"]
            )
            input_file = file

            audio = AudioSegment.from_file(input_file, format="ogg")
            audio.export("output_file.mp3", format="mp3")

            audio_file = open("output_file.mp3", "rb")

            client = openai.AsyncOpenAI(api_key=self.config["api_key"])
            try:
                response = await client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    prompt=self.config["prompt"],
                    temperature=self.config["temperature"],
                )
            except openai.AuthenticationError:
                await utils.answer(message, self.strings["invalid_key"])
                return
            except Exception as e:
                await utils.answer(
                    message,
                    f"<b><emoji document_id=5980953710157632545>‚ùå</emoji>Error: {e}</b>",
                )
                return
            transcription = response.text
            await self.client.edit_message(
                message.chat_id,
                down.id,
                self.strings["recognized"].format(transcription=transcription),
            )
            os.remove(file)
            os.remove("output_file.mp3")
        elif file_extension in [".mp3", "m4a", ".wav", ".mpeg", ".mp4"]:
            await self.client.edit_message(
                message.chat_id, down.id, self.strings["recognition"]
            )
            input_file = file

            audio_file = open(input_file, "rb")

            client = openai.AsyncOpenAI(api_key=self.config["api_key"])
            try:
                response = await client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    prompt=self.config["prompt"],
                    temperature=self.config["temperature"],
                )
            except openai.AuthenticationError:
                await utils.answer(message, self.strings["invalid_key"])
                return
            except Exception as e:
                await utils.answer(
                    message,
                    f"<b><emoji document_id=5980953710157632545>‚ùå</emoji>Error: {e}</b>",
                )
                return
            transcription = response.text
            await self.client.edit_message(
                message.chat_id,
                down.id,
                self.strings["recognized"].format(transcription=transcription),
            )
            os.remove(file)
            os.remove("output_file.mp3")
        else:
            return
